{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset, get_frames_slice_from_scenes\n",
    "from l5kit.dataset import EgoDatasetVectorized\n",
    "from l5kit.vectorization.vectorizer_builder import build_vectorizer\n",
    "from l5kit.data import get_dataset_path\n",
    "from l5kit.sampling.agent_sampling_vectorized import generate_agent_sample_vectorized\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from l5kit.dataset.utils import move_to_device, move_to_numpy\n",
    "from l5kit.visualization.visualizer.zarr_utils import simulation_out_to_visualizer_scene\n",
    "from l5kit.simulation.dataset import SimulationConfig, SimulationDataset\n",
    "from l5kit.simulation.unroll import ClosedLoopSimulator\n",
    "from bokeh.io import output_notebook, show\n",
    "from l5kit.data import MapAPI\n",
    "from l5kit.visualization.visualizer.visualizer import visualize\n",
    "from l5kit.dataset import EgoDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "def inspection(dataset_name=\"train_data_loader\", sample_config=\"/examples/urban_driver/config.yaml\"):\n",
    "    ########################################################################\n",
    "    # Load data and configurations\n",
    "    ########################################################################\n",
    "    # set env variable for data\n",
    "    os.environ[\"L5KIT_DATA_FOLDER\"], project_dir = get_dataset_path()\n",
    "\n",
    "    dm = LocalDataManager(None)\n",
    "\n",
    "    cfg = load_config_data(project_dir + sample_config)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ########################################################################\n",
    "    #  Get the  dataset\n",
    "    ########################################################################\n",
    "    # ===== INIT DATASET\n",
    "    eval_cfg = cfg[\"val_data_loader\"]\n",
    "    eval_zarr = ChunkedDataset(dm.require(eval_cfg[\"key\"])).open()\n",
    "    \"\"\"\n",
    "    ChunkedDataset is a dataset that lives on disk in compressed chunks, it has easy to use data loading and\n",
    "    writing interfaces that involves making numpy-like slices.\n",
    "    Currently only .zarr directory stores are supported (i.e. the data will live in a folder on your\n",
    "    local filesystem called <something>.zarr).\n",
    "    \"\"\"\n",
    "    vectorizer = build_vectorizer(cfg, dm)\n",
    "    eval_dataset = EgoDatasetVectorized(cfg, eval_zarr, vectorizer)\n",
    "    \"\"\"\n",
    "    Get a PyTorch dataset object that can be used to train DNNs with vectorized input\n",
    "    Args:\n",
    "        cfg (dict): configuration file\n",
    "        zarr_dataset (ChunkedDataset): the raw zarr dataset\n",
    "        vectorizer (Vectorizer): a object that supports vectorization around an AV\n",
    "        perturbation (Optional[Perturbation]): an object that takes care of applying trajectory perturbations.\n",
    "    None if not desired\n",
    "    \"\"\"\n",
    "    print(eval_dataset)\n",
    "\n",
    "    ########################################################################\n",
    "    ## Take a look at the data structure\n",
    "    ########################################################################\n",
    "    scene_index = 3\n",
    "    frames = np.ndarray([scene_index])\n",
    "    time_index = 0\n",
    "\n",
    "\n",
    "    # sampled_data = generate_agent_sample_vectorized(time_index, frames, eval_zarr.agents, eval_zarr.tl_faces,\n",
    "    #                                                 None,\n",
    "    #                                                 history_num_frames_ego=1,  # what should we use?\n",
    "    #                                                 history_num_frames_agents=1,\n",
    "    #                                                 future_num_frames=1,  # we must take at least 1 to compute velocity\n",
    "    #                                                 step_time=cfg[\"model_params\"][\"step_time\"],\n",
    "    #                                                 filter_agents_threshold=cfg[\"raster_params\"][\n",
    "    #                                                     \"filter_agents_threshold\"],\n",
    "    #                                                 vectorizer=build_vectorizer(cfg, dm))\n",
    "\n",
    "\n",
    "    ########################################################################\n",
    "    ## Setup the simulator class to be used to unroll the scene\n",
    "    ########################################################################\n",
    "    # ==== DEFINE CLOSED-LOOP SIMULATION\n",
    "    num_simulation_steps = 50\n",
    "    use_agents_gt = False\n",
    "    sim_cfg = SimulationConfig(use_ego_gt=False, use_agents_gt=use_agents_gt, disable_new_agents=True,\n",
    "                             distance_th_far=500, distance_th_close=50, num_simulation_steps=num_simulation_steps,\n",
    "                             start_frame_index=0, show_info=True)\n",
    "    \"\"\"\n",
    "    Defines the parameters used for the simulation of ego and agents around it.\n",
    "\n",
    "        :param use_ego_gt: whether to use GT annotations for ego instead of model's outputs\n",
    "        :param use_agents_gt: whether to use GT annotations for agents instead of model's outputs\n",
    "        :param disable_new_agents: whether to disable agents that are not returned at start_frame_index\n",
    "        :param distance_th_far: if a tracked agent is closed than this value to ego, it will be controlled\n",
    "        :param distance_th_close: if a new agent is closer than this value to ego, it will be controlled\n",
    "        :param start_frame_index: the start index of the simulation\n",
    "        :param num_simulation_steps: the number of step to simulate\n",
    "        :param show_info: whether to show info logging during unroll\n",
    "    \"\"\"\n",
    "\n",
    "    model_path = project_dir + \"/urban_driver_dummy_model.pt\"\n",
    "    model_ego = torch.load(model_path).to(device)\n",
    "    model_ego = model_ego.eval()\n",
    "\n",
    "    model_path = project_dir + \"/urban_driver_dummy_model.pt\"\n",
    "    model_agents = torch.load(model_path).to(device)\n",
    "    model_agents = model_agents.eval()\n",
    "\n",
    "\n",
    "\n",
    "    sim_loop = ClosedLoopSimulator(sim_cfg, eval_dataset, device, model_ego=model_ego, model_agents=model_agents)\n",
    "    \"\"\"\n",
    "       Create a simulation loop object capable of unrolling ego and agents\n",
    "       :param sim_cfg: configuration for unroll\n",
    "       :param dataset: EgoDataset used while unrolling\n",
    "       :param device: a torch device. Inference will be performed here\n",
    "       :param model_ego: the model to be used for ego\n",
    "       :param model_agents: the model to be used for agents\n",
    "       :param keys_to_exclude: keys to exclude from input/output (e.g. huge blobs)\n",
    "       :param mode: the framework that uses the closed loop simulator\n",
    "   \"\"\"\n",
    "\n",
    "    # scenes from the EgoDataset to pick\n",
    "    scene_indices = [0]\n",
    "\n",
    "    torch.set_grad_enabled(False) ########## The unroll gives an error if this is not used-\n",
    "    # RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\n",
    "    # but we can do backprop in time if this is used\n",
    "    # we should probably use VectorizedUnrollModel instead (as in urban_driver/train )\n",
    "\n",
    "    simulated_outputs = sim_loop.unroll(scene_indices, config=cfg)\n",
    "\n",
    "    output_notebook()\n",
    "    mapAPI = MapAPI.from_cfg(dm, cfg)\n",
    "    for sim_out in simulated_outputs:  # for each scene\n",
    "        vis_in = simulation_out_to_visualizer_scene(sim_out, mapAPI)\n",
    "        show(visualize(sim_out.scene_id, vis_in))\n",
    "    \"\"\"\n",
    "    Simulate the dataset for the given scene indices\n",
    "    :param scene_indices: the scene indices we want to simulate\n",
    "    :return: the simulated dataset\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    ########################################################################\n",
    "    #  Transform back from vectorized representation\n",
    "    ########################################################################\n",
    "\n",
    "    ########################################################################\n",
    "    #  Plot initial scene\n",
    "    ########################################################################\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inspection(dataset_name=\"train_data_loader\", sample_config=\"/scenario_generation/config_sample.yaml\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}